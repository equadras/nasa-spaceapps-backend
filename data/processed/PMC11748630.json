{
  "id": "PMC11748630",
  "pmc_link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11748630/",
  "title": "Analyzing the relationship between gene expression and phenotype in space-flown mice using a causal inference machine learning ensemble",
  "authors": "James A. Casaletto, Ryan T. Scott, Makenna Myrick, Graham Mackintosh, Hamed Chok, Amanda Saravia-Butler, Adrienne Hoarfrost, Jonathan M. Galazka, Lauren M. Sanders, Sylvain V. Costes",
  "year": "2025",
  "journal": "Scientific Reports",
  "abstract": "Spaceflight has several detrimental effects on human and rodent health. For example, liver dysfunction is a common phenotype observed in space-flown rodents, and this dysfunction is partially reflected in transcriptomic changes. Studies linking transcriptomics with liver dysfunction rely on tools which exploit correlation, but these tools make no attempt to disambiguate true correlations from spurious ones. In this work, we use a machine learning ensemble of causal inference methods called the Causal Research and Inference Search Platform (CRISP) which was developed to predict causal features of a binary response variable from high-dimensional input. We used CRISP to identify genes robustly correlated with a lipid density phenotype using transcriptomic and histological data from the NASA Open Science Data Repository (OSDR). Our approach identified genes and molecular targets not predicted by previous traditional differential gene expression analyses. These genes are likely to play a pivotal role in the liver dysfunction observed in space-flown rodents, and this work opens the door to identifying novel countermeasures for space travel.",
  "keywords": "Biotechnology, Computational biology and bioinformatics, Biomarkers, Risk factors",
  "full_text": "Introduction Rodent studies demonstrate that spaceflight negatively impacts liver function 1 – 3 . Astronaut studies including the seminal NASA Twins Study also reveal a theme of lipid dysregulation 4 , 5 . Despite these findings, there has been relatively little research studying the impact of microgravity or space radiation on the liver, with more research emphasis on central nervous system effects and carcinogenesis. This is a key knowledge gap considering the disruption of such a critical organ could impact astronaut health and jeopardize the success of future long-term space missions. Identifying the genetic and molecular mechanisms implicated in spaceflight-induced liver dysfunction is required as a first step in precisely mitigating those deleterious effects. Traditional statistical methods identify correlations which may or may not be spurious, especially in high-dimensional, high-throughput data analysis 6 . While randomized controlled trials are considered the gold standard for identifying non-spurious, causal relationships between dependent and independent variables 7 , such experiments can be very expensive and time consuming, or logistically infeasible, especially in a spaceflight environment where sample sizes are limited. Instead, we turn to new machine learning (ML) approaches to identify genes in transcriptomic data predictive of a lipid metabolic response from spaceflight and ground control rodent liver samples. Tools which are commonly used to analyze high-dimensional data, and ML algorithms in general, share an intrinsic flaw. They discover those patterns in data which minimize training error, but training data are often flawed by selection bias, label bias, capture bias, and negative set bias 8 . Algorithms which train on biased datasets inherit these data biases. Minimizing training error encourages algorithms to indiscriminately absorb all the correlations found in training data, real or spurious. Spurious correlations resulting from data biases are unrelated to the true underlying signal 9 . Recently, disambiguating true correlations from spurious ones has been studied in the context of causal inference. For this reason, we leverage tools from the causal inference domain to identify genes which are robustly correlated with a phenotype. While such genes are putatively causal, validating true causality is beyond the scope of this research. In this research, we use CRISP—an ensemble machine learning platform developed by the Frontier Development Laboratory (FDL) 2020 Astronaut Health team 10  to enhance biological and medical research with heterogeneous and high-dimensional observational data 11 . The FDL team used CRISP to identify genetic drivers that differentiate two subtypes of colorectal cancer and to implicate operational taxonomic units of the associated microbiome. The algorithms in the CRISP platform are based on the concept of invariance as a proxy for causal inference. Invariance is a property of a feature which reflects how well a classification algorithm performs using that feature to predict a response invariantly; that is, on data which were generated in different environments, under different circumstances, different conditions, or using different interventions 12 , 13 . An algorithm based on invariance can identify those features that predict the target label regardless of the background data generating processes that gave rise to the dataset. The classic example is a machine learning classifier built to distinguish images of cows from images of camels 14 . A machine learning classifier that is overfit to a particular environment may learn that a cow is an animal that lives in green pastures while a camel is an animal that lives in beige deserts. Given a cow on a sandy beach, this classifier would likely call it a camel. By contrast, a classifier based on invariance would be optimized to ignore the background environment and learn the salient features which truly distinguish a cow from a camel, such as the dimensions of the neck and legs and the shape of the face. Classification algorithms which exploit invariance promote learning correlations that are stable across training environments, as these are expected to persist on out-of-distribution data (i.e. data generated in environments not seen by the algorithm during training) and therefore be robustly correlated to and more likely causal of the response variable 15 . In this research, we leverage several data transformations for augmenting the dataset that additionally provide an environment in which to leverage the CRISP invariance strategy. We use CRISP to identify genes potentially causal of a high lipid density phenotype in space-flown mice liver tissue. We establish a binary threshold of lipid density using scalar values associated with oil red O (ORO) stained tissue. To compare our method with traditional differential gene expression tools, we use EdgeR and DESeq2. We also compare our results with those derived from generic machine learning classifiers including random forest and empirical risk minimization. Overall, we find that CRISP identifies a biologically relevant set of genes which are uniquely predictive of a high lipid density response in space-flown mice. Gene set enrichment and pathway analyses reveal that the dysfunctional regulation of the genes identified by CRISP is implicated in the spectrum of diseases caused by non-alcoholic fatty liver disease (NAFLD). The mice in the experiment flight group were only in space for a maximum of 54 days, yet their gene expression profiles were altered significantly enough to manifest markers of NAFLD. NASA has gathered a significant amount of biomedical data on the effect of short-term spaceflight (< 6 months), from the astronauts of the Apollo missions to those who fly on the International Space Station (ISS). What matters now is not merely the impact of being in space, but rather the impact of living there. Surviving long-term spaceflight (> 6 months) is necessary for the success of the planned Artemis and Mars missions 16 . Our study provides the first machine learning analysis of gene expression predictive of a disease-related response to spaceflight in the liver. Methods and data The data we used for our experiment include transcriptomic and histology data from the liver tissue of space-flown and ground-control mice. The overall workflow for our data and methods is depicted in Fig.  1 . Fig. 1 Experiment setup to compare the most predictive genes of space-flown and ground-control murine lipid density from liver tissue RNA-seq data using both traditional (non-causal inference) methods and the CRISP ensemble of causal inference methods. Gene expression data were transformed to augment the data set and then combined into a single matrix. Environment strings were assigned to each sample based on the name of the transformation performed and the RNA-seq library preparation method. Binary labels (0 and 1) were assigned to each sample based on a threshold value of lipid density. We compare which methods (causal or non-causal inference) perform better at identifying the top genes implicated in the lipid density phenotype. NASA open science data repository The NASA Open Science Data Repository (OSDR) provides AI-ready datasets allowing rapid deployment of machine learning algorithms for data mining. This is possible because OSDR is a FAIR database (Findable, Accessible, Interoperable, Reusable) 16 , 17  with rich metadata providing full context for the data and experiments. The full set of metadata for the samples in our experiment is shown in Supplementary Table  1 . RNA-seq data This study uses transcriptomic data from four OSDR datasets: OSD-47 17  (version 11), OSD-48 17  (version 10), OSD-137 18  (version 6), and OSD-168 17  (version 10). These datasets were generated from three rodent research (RR) missions: RR-1 CASIS, RR-1 NASA, and RR-3. Two different strains of mice were used: C57 and Balb/C. The RR-1 CASIS experiment was designed to study the effects of microgravity of C57 mice on muscle degeneration due to spaceflight (OSD-47). The RR-1 NASA mission was designed to validate the experimental hardware and scientific capabilities on the International Space Station (OSD-48). The RR-3 mission was designed to study countermeasures in Balb/C mice for loss of mass in muscle and bone that have been observed in spaceflight (OSD-137 and OSD-168). The OSD-168 dataset was not based on a separate mission but rather to test the utility of External RNA Control Consortium (ERCC) RNA sequencing controls and therefore constitute technical replicates in our experiments. These rodent research missions were originally designed as randomized controlled experiments, with mice randomly assigned to the groups described in Table  1 . Table 1 The four experimental groups of mice from the OSDR datasets. Group Description Basal Housed in standard vivarium cages on Earth, euthanized 1 day after launch Vivarium Housed in standard vivarium cages on Earth, euthanized n days after launch Ground Housed in ISS habitat cages on Earth, euthanized n days after launch Flight Housed in ISS habitat cages on ISS, euthanized n days after launch The basal, vivarium, and ground groups were combined as “non-flight” samples. In our analyses, we consider the mice in the basal, vivarium, and ground experimental groups as “non-flight” and compare them with the flight group. We are re-using these data to explore the relationship between the transcriptomes and a phenotype, constituting the data as observational in our research. Indeed, the causal inference algorithms in the CRISP platform ensemble were designed to run on observational data. Liver histology phenotype data Liver tissues used for gene expression were quantified for lipid density using the oil red O (ORO) staining protocol. ORO is a fat soluble, hydrophobic dye that stains lipid molecules red 19 . ORO percent positivity was calculated for each sample from the stained images, providing a scalar value that measures the lipid density—higher ORO positivity values directly correspond to higher lipid densities. ORO positivity is the de facto histological biomarker for diagnosing the spectrum of disorders in non-alcoholic fatty liver disease (NAFLD)  post mortem . Because OSD-168 is comprised of technical replicates, the ORO positivity data are associated with the biological replicates in OSD-47, OSD-48, and OSD-137 and not in OSD-168 itself. Data preparation A typical machine learning pipeline includes a data preprocessing step. At the very least, the data must be prepared to satisfy the assumptions and requirements of the algorithms which use the data. CRISP requires that the features be real-valued, that the target be binary, and that the environment string be ASCII text, as described in the following sections. Binarized target The ORO positivity scalar values in our dataset range from 0.91 to 26.94, but the CRISP platform only permits binary targets (low and high) for classification. We converted the scalar value to a per-mission binary value using the mean value between flight and non-flight medians. The thresholds are depicted in the box-and-whisker plot of Fig.  2  as a horizontal dashed blue line. Fig. 2 Box-and-whisker plots for ORO values (y-axis) based on mission (x-axis). The dashed blue line is the rodent research mission threshold that was calculated as the mean of the two group medians (yellow lines). The 2-sided student t-test  p -values show that the differences between the medians of flight and non-flight samples are all significant. The top of each box represents the 75th percentile, the bottom of each box represents the 25th percentile, and the solid black lines on the very top and bottom represent the maxima and minima, respectively. Using the thresholds per mission indicated in Fig.  2 , a sample was assigned a binary target of 0 if its ORO positivity is less than the threshold value and 1 if its ORO positivity is greater than or equal to the threshold value. Feature transformations and data augmentation There are many types of transformations, including power-scaling and normalization, that are commonly performed on data as pre-processing steps in a machine learning pipeline. Gonzalez et al. 20  and others have shown that while data preprocessing is a necessary step in a machine learning pipeline, there isn’t much agreement as to which is the best. Some data transformations are more volatile than others. A data transformation may change the data so drastically that it destroys some of the underlying signals of interest. This lack of data preprocessing standard exists in transcriptomic data analysis as well 21 . In our research, instead of choosing one pre-processing method, we used several methods as shown in Table  2 . This technique of adding differently transformed samples to a dataset is referred to as data augmentation and is a common practice in machine learning 22 . Standardization (converting values to z-scores) is a proven method of data harmonization when combining multiple datasets into one dataset 23 . Each transformation was considered a separate environment across which CRISP must find genes invariantly correlated to the target. We exploit CRISP’s built-in search for invariance across environments and consider each transformation as a perturbation of the data akin to a causal intervention 24 . Table 2 Description of and reference for each gene expression data transformation used in pre-processing of data. Transformation Description Reference Log scale Scales values to their log base 2 Quinn et al. 25 Square root Scales values to their square roots Zhang et al. 26 Median of ratios Scales values to account for sequencing depth, gene length, and outliers Robinson et al. 27 Centered log ratio Transforms data to eliminate over-dispersion Anders et al. 28 Box-Cox Transforms non-normal data into a normal shape Sun et al. 29 Z-score Scales values to their number of standard deviations from the mean Zwiener et al. 30 These 5 transformations (log, square root, median of ratios, centered log ratio, and Box-Cox) provide 6 times more data for building the models and create environments in which to leverage invariance for CRISP. The z-scores of each transformation were individually calculated prior to merging the datasets. Figure  3  shows the original data distribution (named “identity”) and the data after having been transformed and plotted as (variance vs mean) coordinates in log scale. Fig. 3 Scatter plots of variance versus mean for different transformations used in preprocessing. The vertical and horizontal axes are shown in log2 scale after having standardized the data. The identity transformation represents the original, untransformed data. The square root (sqrt) transformation computes the square root of each expression value. The normalization (norm) transformation uses the median of ratios method from the R DESeq2 package. The centered log ratio (clr) method divides each row of gene expression data by the mean of that row and returns the logarithms of those ratios. The log transformation computes the logarithm (base 2) of each expression value. The boxcox transformation is a type of power transformation that makes the gene expression data more normally distributed. The mean–variance plots of the differently transformed data in Fig.  3  reveals that certain transformations change the data significantly while other transformations are relatively mild in effect, compared to the original raw data. In addition to these transformations, we applied some basic filtering of the input to remove transcripts which don’t have ENSEMBL identifiers, don’t code for a protein, have counts of 0 in 80% or more of the samples, have a coefficient of variation less than 0.1, or have counts less than 50 in 80% or more of the samples, as shown in Supplementary Table  6 . This filtering produced the final set of 8,092 genes which we subsequently used in all downstream analysis. To prevent data leakage, we first separated the data into training, validation, and test datasets and standardized the training and validation sets separately 31 . We held out 90% of the raw data for testing and did not perform any transformations on it 32 . We held out 10% of the raw data for validation, and we used all the augmented data during the training phase. Technical batch effects We examined the dataset for batch effects and found that only the type of library preparation of the RNA-seq experiments generating the transcriptomic data clearly separate the samples, as shown in Fig.  4 a. Fig. 4 PCA plots of OSD datasets colorized by the different covariates, including ( a ) library preparation, ( b ) dataset, ( c ) experimental group, ( d ) mouse strain, and ( e ) Rodent Research study. The first two principal components capture 92% of the total variance of the gene expression. Colorizing by library preparation in Fig. 4a shows a distinct separation of all samples. Unlike Fig.  4 a (library preparation), Fig.  4 b–e do not show a clean separation of samples colored by covariate. To account for this library preparation batch effect, we include the library preparation in the environment string, as discussed in the following section. Constructing the environment string CRISP defines the environments of each sample by associating with it an environment string value. Different values of this string represent known perturbations in the data generating process for that sample. In our experiment, the two known perturbations of the data include the library preparation and the data transformation. Because the other covariates do not cluster in the PCA plot, we conclude they do not create a batch effect and therefore are not included in the environment string. Causal inference algorithms which exploit environment invariance theoretically perform better on out-of-distribution data when the number of environments used to train the model is high 12 . However, if the number of samples in the dataset is small (which is intrinsic to transcriptomic experiments), then the environment string must be selected such that there are enough samples in each partition to test for significance. Therefore, we restricted our choice of environment string to include only known perturbations of the data—i.e. transformation and library preparation—and excluded perturbations of unknown effect such as study and group. We show results using strain in the environment string in Supplemental Figs.  4  and  5 . Table  3  shows a snippet of one fictitious sample’s input data after having performed the 5 data transformations and the standardization. The identity transformation is the original data, standardized. The environment string (here, called “env”) is a concatenation of the sample library preparation (here, “polyA”) and the transformation name (e.g. “boxcox”). Only the binary ORO threshold (called “oro_thresh”) for the sample remains unchanged across the same sample as well as for its respective technical replicates of OSD-168, if they exist. As a result of performing these 5 data transformations, our combined dataset size of 51 samples was increased six-fold to 306 samples. Table 3 One fictitious mouse sample’s gene expression data (columns truncated to 2 genes) after 5 data transformations and standardization. Sample Gnai3 Apoh … env oro_thresh Mmus_FLT_I_boxcox − 0.012137 − 0.011788 … polyA:boxcox 1 Mmus_FLT_I_clr 0.969207 3.344341 … polyA:clr 1 Mmus_FLT_I_log − 0.011233 2.079650 … polyA:log 1 Mmus_FLT_I_norm 0.969207 3.344341 … polyA:norm 1 Mmus_FLT_I_sqrt 0.388949 5.967345 … polyA:sqrt 1 Mmus_FLT_I_identity − 0.011230 2.079653 … polyA:identity 1 The environment string (“env”) includes the name of the library preparation (“polyA” or “ribo-depleted”) and the name of the transformation. The binary ORO threshold (“oro_thresh”) is set to either 0 (low ORO positivity) or 1 (high ORO positivity). Running CRI",
  "introduction": "Introduction Rodent studies demonstrate that spaceflight negatively impacts liver function 1 – 3 . Astronaut studies including the seminal NASA Twins Study also reveal a theme of lipid dysregulation 4 , 5 . Despite these findings, there has been relatively little research studying the impact of microgravity or space radiation on the liver, with more research emphasis on central nervous system effects and carcinogenesis. This is a key knowledge gap considering the disruption of such a critical organ could impact astronaut health and jeopardize the success of future long-term space missions. Identifying the genetic and molecular mechanisms implicated in spaceflight-induced liver dysfunction is required as a first step in precisely mitigating those deleterious effects. Traditional statistical methods identify correlations which may or may not be spurious, especially in high-dimensional, high-throughput data analysis 6 . While randomized controlled trials are considered the gold standard for identifying non-spurious, causal relationships between dependent and independent variables 7 , such experiments can be very expensive and time consuming, or logistically infeasible, especially in a spaceflight environment where sample sizes are limited. Instead, we turn to new machine learning (ML) approaches to identify genes in transcriptomic data predictive of a lipid metabolic response from spaceflight and ground control rodent liver samples. Tools which are commonly used to analyze high-dimensional data, and ML algorithms in general, share an intrinsic flaw. They discover those patterns in data which minimize training error, but training data are often flawed by selection bias, label bias, capture bias, and negative set bias 8 . Algorithms which train on biased datasets inherit these data biases. Minimizing training error encourages algorithms to indiscriminately absorb all the correlations found in training data, real or spurious. Spurious correlations resulting from data biases are unrelated to the true underlying signal 9 . Recently, disambiguating true correlations from spurious ones has been studied in the context of causal inference. For this reason, we leverage tools from the causal inference domain to identify genes which are robustly correlated with a phenotype. While such genes are putatively causal, validating true causality is beyond the scope of this research. In this research, we use CRISP—an ensemble machine learning platform developed by the Frontier Development Laboratory (FDL) 2020 Astronaut Health team 10  to enhance biological and medical research with heterogeneous and high-dimensional observational data 11 . The FDL team used CRISP to identify genetic drivers that differentiate two subtypes of colorectal cancer and to implicate operational taxonomic units of the associated microbiome. The algorithms in the CRISP platform are based on the concept of invariance as a proxy for causal inference. Invariance is a property of a feature which r",
  "methods": "CRISP ensemble methods CRISP is an ensemble of machine learning methods which have been designed to identify features causal of a target 11 . We leveraged CRISP to identify genes potentially causal of a phenotype. There are 2 types of algorithms in CRISP: one based on invariant causal prediction (ICP) 13 ; and one based on invariant risk minimization (IRM) 12 . There is a linear and non-linear version of each of these algorithms. The linear versions use linear combinations of the feature space to construct a binary output, and the non-linear versions use non-linear combinations (multi-layer perceptrons with non-linear activation functions) of the feature space to construct a binary output. Both algorithms rely on invariance as a proxy for causality, and invariance in turn relies on having data partitioned by known interventions or perturbations. Each of these methods trains a model using a training set, selects model hyper-parameters using a validation set, and provides a final model accuracy on a held-out test set. CRISP partitions the data into different environments such that the model is trained on data from one environment and then validated and tested on the data from the other environments. CRISP uses two methods of feature reduction to reduce the number of variables to search. The first method removes features whose correlation to the target is below a certain threshold, and the second method uses a multi-layer perceptron to identify the most salient features based on model feature importance.",
  "results": "Analyzing results from empirical risk minimization Following the same analysis as with random forest, here we analyze the results from empirical risk minimization (ERM). ERM uses the canonical minimization of the sum of the squares of the residuals as its unconditional objective function. It does not partition the data into environments like IRM. As such, it is perhaps the closest comparison to the linear IRM and non-linear IRM results from CRISP. Among the 20 genes that ERM identified as predictive of the lipid density response, none of them are involved in the lipid metabolic process according to NCBI. Neither the ShinyGO nor the GSEA enrichment tools found any overlapping pathways or gene ontologies using these 20 genes as input. We conclude that the ERM classifier found spurious correlations between the expression of those 20 genes and the lipid density response. The 20 genes which ERM found as most predictive of the lipid density phenotype are shown in Supplementary Table  5 .",
  "discussion": "Discussion The CRISP platform trains an ensemble of binary classifiers on data in one environment and validates and tests them using the data from the other environments. Separating the environments in this way ensures that the ensemble algorithms are rewarded for finding correlations that are independent of biases in the data generating processes. Classifying data independently of the environment is the definition of invariance in this context and leads to more robustly correlated results. Using the CRISP ensemble, we’ve identified a set of genes which are predictive of high and low lipid density. These genes are significantly associated with gene sets and genetic pathways that are consistent with NAFLD or similar liver dysfunction. On the other hand, machine learning methods such as empirical risk minimization and random forest do not distinguish between spurious and non-spurious correlations. In our experiment, their top 20 genes are not biologically related to lipid metabolism, despite the random forest classifier having the highest held-out test accuracy (100%) of all the models in predicting lipid density. The DESeq2 package did not find any genes significantly correlated to our phenotype. The EdgeR package found 6 genes whose expression was significantly correlated to the lipid density response variable—one of which ( Star ) was also found by CRISP. However, gene set enrichment and pathway analysis tools found a wide variety of conditions and processes associated with these 6 genes, leaving the results difficult to interpret. We attribute these quantitative and qualitative differences in results to the environment invariance modeling approach that the CRISP ML algorithms use in conjunction with the data augmentation methods we leveraged. As opposed to the non-specific pathways enriched by the EdgeR genes, the CRISP genes enrich lipid metabolism pathways including cholesterol, fatty acid, and NAFLD pathways—all of which are consistent with using lipid density as a response in our machine learning models. The NAFLD pathway is significantly enriched by the CRISP gene result set and includes  Cyp2e1 ,  Fasn , and  Scd1  genes with a 0.014 strength of association, as shown in Table  6 . As noted in Table  8 , the literature is consistent in finding these genes implicated in experiments on NAFLD. Moreover, the SPOKE knowledge graph found MASLD (the new name for NAFLD) to be the most significantly associated disease ontology with the CRISP gene result set. The use of data augmentation in this CRISP experiment is crucial. Not only does augmentation provide known perturbations of the data as required by CRISP, data augmentation also provides more samples to train, test, and validate the ensemble of machine learning classifiers. Even so, having more samples than 51 would enrich the underlying gene expression signals which would lead to even more robust correlations. We therefore recommend future spaceflight experiments to increase the cohort size an",
  "conclusion": ""
}